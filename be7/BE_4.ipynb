{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv0CIdQE3XXG"
   },
   "source": [
    "# Fiabilité et propagation d'incertitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1Oo6NYb3XXN"
   },
   "source": [
    "# 0) Import des bibliothèques Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "mPll_tTZ3aJa",
    "outputId": "1ea14c8c-8de1-421f-bfb7-138b681c5577"
   },
   "outputs": [],
   "source": [
    "!pip install openturns \n",
    "!pip install SMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wm8cloed3XXP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "import openturns as ot\n",
    "import scipy.stats as stat\n",
    "import scipy.optimize as opt\n",
    "import time as t #utile pour mesurer le temps d'execution des fonctions\n",
    "import matplotlib.pyplot as plt\n",
    "from smt.surrogate_models import KRG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmhOdm9E3XXY"
   },
   "source": [
    "# 1) Description du problème\n",
    "## 1-1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6kHWQFu3XXa"
   },
   "source": [
    "On souhaite étudier la fiabilité d'une passerelle soumise à un chargement réparti sur son tablier. Pour cela, la passerelle est modélisée par une poutre de longueur $L$, encastrée à son extrémité gauche et en appui simple à son extrémité droite, le chargement est noté $Q(l)$. La passerelle est de section rectangulaire de base $b$ et de hauteur $h$. Le module d'élasticité du matériau servant à la construction de la passerelle est $E$. La défaillance est caractérisée par un déplacement maximal autorisé noté $u_{max}$. \n",
    "\n",
    "La résolution de l'équation de la statique permettant de trouver le déplacement de la passerelle est effectuée par la méthode des éléments finis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZx0EOMA3XXc"
   },
   "source": [
    "## 1-2) Résolution par la méthode des éléments finis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mo3HfHEl3XXe"
   },
   "source": [
    "Le nombre d'éléments est noté $n_{elem}$. On suppose que le chargement réparti est vertical et compte $n_{nodes}=n_{elem}+1$ composantes notées $Q_i,~i=1,\\cdots,n_{nodes}$. Le code suivant crée la fonction Python qui résout l'équation de la statique par la méthode des éléments finis et renvoie le déplacement max de la structure. Pour cela $n_{elem}$ éléments de poutre à 2 degrés de liberté par noeud sont utilisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "py7wQJoo3XXg"
   },
   "outputs": [],
   "source": [
    "def u_structure(n_elem,L,E,I,Q):\n",
    "    \n",
    "        #matrice de rigidité totale\n",
    "        K = np.zeros((2*(n_elem+1),2*(n_elem+1)))\n",
    "        #Longeur des éléments\n",
    "        l = L/n_elem\n",
    "        #boucle sur les éléments\n",
    "        for i in range(n_elem):\n",
    "            #matrice élémentaire\n",
    "            K_elem = E*I/l**3*np.array([[12.0,6.0*l,-12.0,6.0*l],\\\n",
    "                                        [6.0*l,4.0*l**2,-6.0*l,2.0*l**2],\\\n",
    "                                       [-12.0,-6.0*l,12.0,-6.0*l],\\\n",
    "                                       [6*l,2.0*l**2,-6.0*l,4.0*l**2]])\n",
    "            #Assemblage\n",
    "            K[2*i:2*(i+1),2*i:2*(i+1)] = K[2*i:2*(i+1),2*i:2*(i+1)] + K_elem[0:2,0:2]\n",
    "            K[2*(i+1):2*(i+2),2*i:2*(i+1)] = K[2*(i+1):2*(i+2),2*i:2*(i+1)] + K_elem[2:4,0:2]\n",
    "            K[2*i:2*(i+1),2*(i+1):2*(i+2)] = K[2*i:2*(i+1),2*(i+1):2*(i+2)] + K_elem[0:2,2:4]\n",
    "            K[2*(i+1):2*(i+2),2*(i+1):2*(i+2)] = K[2*(i+1):2*(i+2),2*(i+1):2*(i+2)] + K_elem[2:4,2:4]\n",
    "            #conditions aux limites\n",
    "        #encastrement en l = 0\n",
    "        K[0:2,:] = 0.0\n",
    "        K[:,0:2] = 0.0\n",
    "        K[0,0] = 1.0\n",
    "        K[1,1] = 1.0\n",
    "        #appui simple en l = L\n",
    "        K[-2,:] = 0.0\n",
    "        K[:,-2] = 0.0\n",
    "        K[-2,-2] = 1.0\n",
    "        #Decomposition de Cholesky \n",
    "        K_fact = sci.linalg.cho_factor(K)\n",
    "        #Second membre\n",
    "        F = np.zeros((2*(n_elem+1),))\n",
    "        F[0:-1:2] = Q    \n",
    "        F[0:2] = 0.0\n",
    "        F[-2] = 0.0\n",
    "        #resolution du système KU = F\n",
    "        U = sci.linalg.cho_solve(K_fact,F)    \n",
    "        return U\n",
    "\n",
    "def interpolation_EF(X,U,L):\n",
    "    n_elem = len(U)/2-1\n",
    "    l = L/n_elem\n",
    "    U_X = np.zeros(X.shape)\n",
    "    i = 0\n",
    "    for x in X:\n",
    "        #on cherche à quel élément appartient x et quelle est sa position relative\n",
    "        n = np.int((x-1e-12)/l) #on se décale très légèrement sur la gauche \n",
    "        #pour ne pas avoir de problème en x=L qui donnerait n=en_elem +1 et qui poserait problème\n",
    "        s = (x-(n*l))/l\n",
    "        #On recupère U_i et U_j correspondant\n",
    "        u_i = U[2*n]\n",
    "        theta_i = U[2*n+1]\n",
    "        u_j = U[2*(n+1)]\n",
    "        theta_j = U[2*(n+1)+1]\n",
    "        u_x = u_i*(1.0-3.0*s**2+2.0*s**3)+u_j*(3.0*s**2-2.0*s**3)\\\n",
    "        +theta_i*(l*(s-2.0*s**2+s**3))+theta_j*(l*(-s**2+s**3))\n",
    "        U_X[i] = u_x\n",
    "        i = i+1   \n",
    "    return U_X    \n",
    "\n",
    "def find_max_dep(n_elem,L,E,I,Q):\n",
    "    #On commence par résoudre le problème discrétisé\n",
    "    U = u_structure(n_elem,L,E,I,Q)\n",
    "    #Puis on cherche le min du problème continu grace à la fonction d'interpolation\n",
    "    res_opt = opt.minimize(interpolation_EF,np.array([L/2.0]),method = \"SLSQP\", bounds=[[0.0,L]],args=(U,L),tol=1e-12)\n",
    "    u_max = res_opt['fun']\n",
    "    l_max = res_opt['x']\n",
    "    return l_max,u_max,U    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lqWmkfE63XXj"
   },
   "source": [
    "<div class=\"alert alert-danger \"><b>  Dans la suite on fixera $L=10.0$, $b=1.0$, $h=0.1$, $E =12e^9$, $Q_{total} = -3000$, $n_{elem}=50$ </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTeS00J13XXy"
   },
   "source": [
    "# 2) Propagation d'incertitude et analyse de sensibilité\n",
    "\n",
    "On suppose à présent que les varibales $b$ et $E$ sont aléatoires. L'objectif est de modéliser ces variables incertaines puis de faire une analyse de sensibilité du déplacement. Pour cela nous calculerons les indices de sensibilité basés sur la décomposition de la variance (indice de Sobol). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zDDiY6F13XXz"
   },
   "source": [
    "## 2-1) Modèle probabiliste des variables $b$ et $E$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BJNr6sF3XX0"
   },
   "source": [
    "Dans la suite on suppose que le code EF ci dessus donne la **valeur exacte** du déplacement maximal de la passerelle.\n",
    "Dans cet exercice nous allons considérer que les paramètres $E$ et $b$ sont des variables aléatoires indépendantes telles que :\n",
    "- $E$ suit une loi log-normale de moyenne $\\mu_E=12e9$ et de coefficient de variation $10\\%$ http://openturns.github.io/openturns/latest/user_manual/_generated/openturns.LogNormal.html?highlight=lognormal. Remarque : $X$ de loi log-normale, $\\mu_X= E[X]$, $\\sigma_X=\\sqrt{Var[X]}$, il existe une unique variable $Y$ de loi normale telle que $ln(X) = Y$ de moyenne $\\mu_Y=\\lambda_X$ et d'ecart type $\\sigma_Y=\\xi_X$ et nous avons\n",
    "$$\n",
    "\\lambda_X = ln\\left( \\frac{\\mu_X}{\\sqrt{1+(\\frac{\\sigma_X}{\\mu_X})^2}}\\right)\n",
    "$$\n",
    "$$\n",
    "\\xi_X = \\sqrt{ ln(1+(\\frac{\\sigma_X}{\\mu_X})^2)}\n",
    "$$\n",
    "\n",
    "- $b$ suit une loi uniforme sur $[0.9,1.1]$ http://openturns.github.io/openturns/latest/user_manual/_generated/openturns.Uniform.html?highlight=uniform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJpevyBP3XX1"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\"> Le code ci dessous créer le modèle probabiliste des variables $E$ et $b$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jd1lY_ZC3XX2",
    "outputId": "4965ca07-7d5f-4e5e-9a43-4cb0962e1ac9"
   },
   "outputs": [],
   "source": [
    "mean_E = 12e9\n",
    "cv_E = 0.1\n",
    "Lambda_E = np.log(mean_E/(np.sqrt(1.+cv_E**2)))\n",
    "Xi_E = np.sqrt(np.log(1.+cv_E**2))\n",
    "Loi_E = ot.LogNormal(Lambda_E,Xi_E)\n",
    "print (\"mu_E =\",Loi_E.getMean()[0],\", cv_E=\", Loi_E.getStandardDeviation()[0]/Loi_E.getMean()[0])\n",
    "b_inf = 0.9\n",
    "b_sup = 1.1\n",
    "Loi_b = ot.Uniform(0.9,1.1)\n",
    "print (\"mu_b=\",Loi_b.getMean()[0],\", cv_b=\", Loi_b.getStandardDeviation()[0]/Loi_b.getMean()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPqN9Icz3XX5"
   },
   "source": [
    "## 2.2) Propagation d'incertitude par chaos polynomial\n",
    "L'objectif de cette partie est de construire une approximation par chaos polynomial de la fonction $U_{max}(E,b)$.\n",
    "\n",
    "$$\n",
    "U_{max}(E,b) \\approx \\hat{U}_{max}(E,b)=\\sum_{i=1}^P \\alpha_i \\phi_i(E,b)  \n",
    "$$\n",
    "\n",
    "Pour cela nous utilisons l'objet FunctionalChaosAlgorithm d'openturns (http://openturns.github.io/openturns/latest/user_manual/response_surface/_generated/openturns.FunctionalChaosAlgorithm.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrB7F2mL6gLb"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 1)** :  Comment est construite la base de polynômes $\\Phi=\\left\\lbrace \\phi_i(E,b) \\right\\rbrace_{i\\in\\mathbb{N}}$? Quelles sont les familles de polynômes utilisées? \n",
    "\n",
    "Vérifier votre réponse en complétant le code ci-dessous à l'aide de la fonction StandardDistributionPolynomialFactory (http://openturns.github.io/openturns/latest/user_manual/_generated/openturns.StandardDistributionPolynomialFactory.html#openturns.StandardDistributionPolynomialFactory) \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "JDYwjpOm3XX7",
    "outputId": "46d3fcb3-9568-434e-b300-4376d84f08e6"
   },
   "outputs": [],
   "source": [
    "#construction de la base de polynômes\n",
    "#polynomes orthogonaux par rapport à E\n",
    "P_E = ot.#A Completer\n",
    "print(P_E)\n",
    "#polynomes orthogonaux par rapport à b\n",
    "P_b = #A Completer\n",
    "print(P_b)\n",
    "#tensorisation des deux familles\n",
    "base = ot.OrthogonalProductPolynomialFactory([P_E,P_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPJJVT2Z3XX-"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 2) : ** Proposer une solution afin d'eviter le calcul numérique de la familles de polynômes orthogonaux par rapport à la la loi de $E$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1BeSyas-mMN"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 3) : ** On souhaite que la base $\\Phi$ contienne tous les polynômes jusqu'au degré 3, déterminer la valeur de $P$. *(indice : np.math.factorial())*</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CUvr7Kix_H_w",
    "outputId": "ec40536f-0573-4077-9710-4a1326f8295e"
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "d = 3\n",
    "P = int(#A Completer)\n",
    "print ('P=',P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOuqqrpP_kDA"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 4) : ** Nous proposons d'utiliser le calcul des coefficients par moindre carrée, pour cela on utilisera un échantillon de taille $3P$. Compléter le code suivant afin de créer cet échantillon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ME4vtPejAFLU"
   },
   "outputs": [],
   "source": [
    "##génération d'échantillons\n",
    "n_sim = #A Completer\n",
    "Echantillon_E = np.array(#A Completer)\n",
    "Echantillon_b = np.array(#A Completer)\n",
    "##paramètres\n",
    "L = 10.0 #longueur en métre\n",
    "h = 0.1 #hauteur de la section en métre\n",
    "#Chargement : \n",
    "#Chargement total\n",
    "Q_total = -3000.0 #Newton\n",
    "#Le chargement est réparti uniformement sur les noeuds entre les 2 extremitées\n",
    "n_elem = 50\n",
    "n_nodes = n_elem+1\n",
    "Q_elem = Q_total/n_elem\n",
    "Q = np.ones((n_nodes,))*Q_elem\n",
    "#Vecteur U_max\n",
    "U_max = np.zeros((n_sim,))\n",
    "##Exécution du code EF pour chaque échantillon\n",
    "for i in range(n_sim):\n",
    "    E = Echantillon_E[i]\n",
    "    b = Echantillon_b[i]\n",
    "    I = b*h**3/12.0\n",
    "    l_max,u_max,U = #A Completer\n",
    "    U_max[i] = #A Completer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b21HCOvdCOJM"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 5) : ** Completer le code suivant qui créer l'approximation par chaos polynômial à l'aide d'openturns. Pour cela utiliser la doc http://openturns.github.io/openturns/latest/user_manual/response_surface/_generated/openturns.FunctionalChaosAlgorithm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "p3-Gj27OCr_E",
    "outputId": "6c9fbcb4-22a1-4455-c9cb-02978b9b0aad"
   },
   "outputs": [],
   "source": [
    "input_distribution = ot.ComposedDistribution([#A Completer])\n",
    "adaptiveStrategy = ot.FixedStrategy(#A Completer,#A Completer)\n",
    "input_sample = np.concatenate((Echantillon_E,Echantillon_b),axis=1)\n",
    "output_sample= U_max.reshape((3*P,1))\n",
    "#Par defaut la méthode utilise les moindres carrés, il n'est donc pas utile de spécifier la projection strategy\n",
    "CP = ot.FunctionalChaosAlgorithm(#A Completer,#A Completer,#A Completer,#A Completer)\n",
    "CP.run()\n",
    "result_CP = CP.getResult()\n",
    "print(result_CP.getCoefficients())\n",
    "print(result_CP.getComposedMetaModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0AqipAaF4n8"
   },
   "source": [
    "Maintenant que l'approximation par chaos polynomial est créée il est possible de l'exploiter directement à l'aide des resultats (getResult) ou en créant un métamodèle (result.getMetaModel()) ou à l'aide d'un FunctionalChaosRandomVector (http://openturns.github.io/openturns/latest/user_manual/response_surface/_generated/openturns.FunctionalChaosRandomVector.html?highlight=functionalchaosrandomvector#openturns.FunctionalChaosRandomVector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vu8j2vEPGb_j"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 6) : ** Compléter le code suivant permettant de tester l'approximation obtenue à l'aide d'un echantillon de validation. En utilisant l'objet MetaModelValidation d'openTurns calucler l'indicateur Q2. Que représente cette valeur? Que pouvez vous conclure sur la qualité de votre métamodèle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "OiVjYTzdF26O",
    "outputId": "bbf92b55-3f89-473f-a942-4e966d13294c"
   },
   "outputs": [],
   "source": [
    "n_validation = 10\n",
    "Echantillon_validation = input_distribution.getSample(n_validation)\n",
    "U_max = np.zeros((n_validation,1))\n",
    "##Exécution du code EF pour chaque échantillon\n",
    "for i in range(n_validation):\n",
    "    E = Echantillon_validation[i,0]\n",
    "    b = Echantillon_validation[i,1]\n",
    "    I = b*h**3/12.0\n",
    "    l_max,u_max,U = find_max_dep(n_elem,L,E,I,Q)\n",
    "    U_max[i] = u_max\n",
    "Meta_model = result_CP.#A Completer\n",
    "U_CP = Meta_model(Echantillon_validation)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel(r'$u_{max}$',fontsize = 30)\n",
    "ax.set_ylabel (r'$\\hat{u}_{max}$',fontsize=30)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.plot(U_max,U_CP,'+',markersize= 30) \n",
    "\n",
    "#Q2 predictivity factor\n",
    "# Validation of the model\n",
    "val = ot.MetaModelValidation#A COMPLETER\n",
    "# Compute the first indicator : q2\n",
    "q2 = #A COMPLETER\n",
    "print('Predictivity factor ', q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOwOVQ99H1f5"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 7) : ** Estimer la moyenne et la variance de $U_{max}$ à l'aide de l'approximation par CP. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0H14PD3I5-q"
   },
   "outputs": [],
   "source": [
    "coefs = np.array(result_CP.#A Completer)\n",
    "print(\"Mean=\", #A Completer)\n",
    "print(\"Variance=\", #A Completer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gS1lv9gIZ7-"
   },
   "source": [
    "Nous allons à present observer la convergence de la moyenne et de la variance en fonction du degré de l'approximation.\n",
    "\n",
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 8) : ** Que pouvez vous en conclure sur la fonction $U_{max}(E,b)$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "8LjgYm0uInGM",
    "outputId": "35df6b55-2efd-4ffa-8c03-32567c8e5631"
   },
   "outputs": [],
   "source": [
    "Mean = []\n",
    "Var = []\n",
    "D = [0,1,2,3,4,5]\n",
    "for d in D:\n",
    "  P = int(np.math.factorial(n+d)/(np.math.factorial(n)*np.math.factorial(d)))\n",
    "  adaptiveStrategy = ot.FixedStrategy(base,P)\n",
    "  CP = ot.FunctionalChaosAlgorithm(input_sample,output_sample,input_distribution,adaptiveStrategy)\n",
    "  CP.run()\n",
    "  result_CP = CP.getResult()\n",
    "  coefficients = np.array(result_CP.getCoefficients())\n",
    "  Mean.append(coefficients[0])\n",
    "  Var.append(np.sum(coefficients[1:]**2))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel(r'$d$',fontsize = 30)\n",
    "ax.set_ylabel (r'$\\mathbb{E}[\\hat{U}_{max}]$',fontsize=30)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.plot(D,Mean, label = 'Moyenne')\n",
    "ax.legend(loc=0)\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel(r'$d$',fontsize = 30)\n",
    "ax.set_ylabel (r'$Var[\\hat{U}_{max}]$',fontsize=30)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.plot(D,Var, label = 'Variance')\n",
    "ax.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZrTePmhIkw1"
   },
   "source": [
    "## 2.3) Analyse de sensibilité \n",
    "\n",
    "Nous allons à présent exploiter l'approximation par chaos polynomial afin d'estimer les indices de Sobol. Avant cela nous allons, à l'aide de simulations, décrire un peu plus le principe de ces indices. \n",
    "\n",
    "Dans notre cas d'application à deux variables, nous avons 3 indices, celui de la variable $E$, celui de la variable $B$ et celui caractérisant l'intéraction entre $E$ et $B$. Ce cas simple à deux variables n'est pas très réaliste pour une analyse de sensibilité mais à l'avantage d'être illustratif.\n",
    "\n",
    "### 2.3.1) Espérance conditionnelle \n",
    "\n",
    "On rappelle que l'indice de Sobol  de la variable $X_i$ est donnée par :\n",
    "\n",
    "$$\n",
    "S_i = \\frac{Var[\\mathbb{E}[Y|X_i]]}{Var[Y]}\n",
    "$$\n",
    "\n",
    "Nous allons donc, dans un premier temps, étudier par simulation la variable aléatoire $\\mathbb{E}[Y|X_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 9) : ** Compléter le code suivant afin de tracer les histogrammes de $\\mathbb{E}[U_{max}|E]$ et $\\mathbb{E}[U_{max}|B]$. Pour les simulations nous utiliserons l'approximation par chaos polynomial validée précédemment. Expliquer le rôle des variables n_sim_1 et n_sim_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##génération d'échantillons\n",
    "n_sim = 3*P\n",
    "Echantillon_E = np.array(Loi_E.getSample(n_sim))\n",
    "Echantillon_b = np.array(Loi_b.getSample(n_sim))\n",
    "##paramètres\n",
    "L = 10.0 #longueur en métre\n",
    "h = 0.1 #hauteur de la section en métre\n",
    "#Chargement : \n",
    "#Chargement total\n",
    "Q_total = -3000.0 #Newton\n",
    "#Le chargement est réparti uniformement sur les noeuds entre les 2 extremitées\n",
    "n_elem = 50\n",
    "n_nodes = n_elem+1\n",
    "Q_elem = Q_total/n_elem\n",
    "Q = np.ones((n_nodes,))*Q_elem\n",
    "#Vecteur U_max\n",
    "U_max = np.zeros((n_sim,))\n",
    "##Exécution du code EF pour chaque échantillon\n",
    "for i in range(n_sim):\n",
    "    E = Echantillon_E[i]\n",
    "    b = Echantillon_b[i]\n",
    "    I = b*h**3/12.0\n",
    "    l_max,u_max,U = find_max_dep(n_elem,L,E,I,Q)\n",
    "    U_max[i] = u_max\n",
    "input_distribution = ot.ComposedDistribution([Loi_E,Loi_b])\n",
    "adaptiveStrategy = ot.FixedStrategy(base,P)\n",
    "input_sample = np.concatenate((Echantillon_E,Echantillon_b),axis=1)\n",
    "output_sample= U_max.reshape((3*P,1))\n",
    "#Par defaut la méthode utilise les moindres carrés, il n'est donc pas utile de spécifier la projection strategy\n",
    "CP = ot.FunctionalChaosAlgorithm(input_sample,output_sample,input_distribution,adaptiveStrategy)\n",
    "CP.run()\n",
    "result_CP = CP.getResult() \n",
    "Meta_model = result_CP.getMetaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim_1 = 500\n",
    "n_sim_2 = 200\n",
    "#Esperance de U_max sachant E\n",
    "E_U_E = []\n",
    "sample_E = #A Completer\n",
    "for i in range(n_sim_1):\n",
    "    sample_B = #A Completer\n",
    "    X = np.zeros((n_sim_2,2))\n",
    "    X[:,0]=#A Completer\n",
    "    X[:,1]=#A Completer\n",
    "    U_e = Meta_model(X)\n",
    "    E_U_E.append(#A Completer)\n",
    "E_U_E = np.array(E_U_E)\n",
    "\n",
    "\n",
    "#Esperance de U_max sachant B\n",
    "E_U_B = []\n",
    "sample_B = #A Completer\n",
    "for i in range(n_sim_1):\n",
    "    sample_E = #A Completer\n",
    "    X = np.zeros((n_sim_2,2))\n",
    "    X[:,0]=#A Completer\n",
    "    X[:,1]=#A Completer\n",
    "    U_b = Meta_model(X)\n",
    "    E_U_B.append(#A Completer)\n",
    "E_U_B = np.array(E_U_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel(r'$\\hat{u}_{max}$',fontsize = 30)\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.hist(E_U_E, label = r'$\\mathbb{E}[\\hat{U}_{max}|E]$', density=True, alpha = 0.5)\n",
    "ax.hist(E_U_B, label = r'$\\mathbb{E}[\\hat{U}_{max}|B]$', density=True, alpha = 0.5)\n",
    "ax.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 10) : ** A l'aide des histogrammes pouvez-vous conclure sur l'analyse de sensibilité du problème? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 11) : ** A l'aide des simulations précédentes donner une estimation des indices de sensibilité. Pour cela vous completerez le code suivant. Quelle est l'influence de n_sim_1 et n_sim_2. Ces estimations sont-elles cohérentes avec vos conclusions précédentes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_E = #A Completer\n",
    "S_B = #A Completer\n",
    "print(\"Indice $S_E$=\", S_E)\n",
    "print(\"Indice $S_B$=\", S_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2) Estimation des indices de sensibilité en exploitant les coefficients de l'approximation par chaos polynomial\n",
    "\n",
    "Nous allons à présent estimer les indices de sensibilité à l'aide des formules vues en cours. Pour cela, nous utiliserons les fonctions déjà codées dans la bibliothèque openturns https://openturns.github.io/openturns/latest/user_manual/response_surface/_generated/openturns.FunctionalChaosSobolIndices.html?highlight=functionalchaossobolindices#openturns.FunctionalChaosSobolIndices\n",
    "\n",
    "et \n",
    "\n",
    "https://openturns.github.io/openturns/latest/auto_meta_modeling/polynomial_chaos_metamodel/plot_chaos_ishigami.html?highlight=functionalchaossobolindices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 12) : ** Compléter le code suivant afin d'effectuer l'analyse de sensibilité en exploitant l'approximation par chaos polynomial validée précédemment. Les résultats sont-ils en accord avec vos estimations précédentes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chaosSI = ot.#A Completer\n",
    "print(chaosSI.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to draw the indices \n",
    "import openturns.viewer as viewer\n",
    "from matplotlib import pylab as plt\n",
    "input_names = ['E', 'b']\n",
    "n = 2\n",
    "first_order = [chaosSI.getSobolIndex(i) for i in range(n)]\n",
    "total_order = [chaosSI .getSobolTotalIndex(\n",
    "    i) for i in range(n)]\n",
    "graph = ot.SobolIndicesAlgorithm.DrawSobolIndices(\n",
    "    input_names, first_order, total_order)\n",
    "view = viewer.View(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmwZD8ZlK8bw"
   },
   "source": [
    "## 2.4) Fiabilité par approximation par chaos polynomial\n",
    "On rappele que le problème de fiabilité est définie par la fonction de performance \n",
    "$$\n",
    "G(X) = d_{seuil}-|u_{max}(X)|\n",
    "$$\n",
    "avec $d_{seuil}=0.022$.\n",
    "\n",
    "<p class=\"bg-primary\" style=\"padding:1em\">**Question 13) : ** Proposer une approche afin d'estimer la probablité de défaillance à l'aide de l'approximation par CP? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MNX8sMxFLzg_",
    "outputId": "68f455c4-fbe0-4391-f824-b4d261b1a83a"
   },
   "outputs": [],
   "source": [
    "#A Completer\n",
    "print(\"Pf CP=\",Pf)\n",
    "print(\"Cv PF=\",CV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pR_HLg3NMqY"
   },
   "source": [
    "<p class=\"bg-primary\" style=\"padding:1em\">** **Question Bonus : ** Le coefficient de variation calculé précédemment ne prend en compte que la variation du plan de Monte Carlo mais pas l'approximation par CP. Faite une recherche sur le ré échantillonnage bootstrap et proposer une approche pour quantifier l'incertitude liée à l'approximation par chaos polynomial. On pourra appliquer cette démarche à l'estimation des indices de sensibilité et à l'estimation de la probabilité de défaillance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BE_2_corrige.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
